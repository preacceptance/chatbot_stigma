{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def read_json(fname):\n",
    "    with open(fname) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def process():\n",
    "    df = pd.read_csv('./data/data.csv')\n",
    "\n",
    "    chatlogs = pd.read_csv('./data/chatlogs.csv')\n",
    "    messages = pd.read_csv('./data/messages.csv')\n",
    "\n",
    "    def get_appended_human_messages(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            human = messages[(messages['worker_id'] == row['mturk_id']) & (~pd.isna(messages['user_id']))]['content'].values.tolist()\n",
    "            human = [h for h in human if type(h) == str]\n",
    "        else:\n",
    "            # Get room id\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[0]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "\n",
    "            human = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "            human = [h for h in human if type(h) == str]\n",
    "\n",
    "\n",
    "        #return \"Hidden\"\n",
    "        return \" \".join(human)\n",
    "\n",
    "    def get_appended_ai_messages(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            ai = messages[(messages['worker_id'] == row['mturk_id']) & (pd.isna(messages['user_id']))]['content'].values.tolist()\n",
    "            ai = [h for h in ai if type(h) == str]\n",
    "        else: # Talking to a real person, get the pair's messages\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[1]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "\n",
    "            ai = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "            ai = [h for h in ai if type(h) == str]\n",
    "\n",
    "\n",
    "        #return \"Hidden\"\n",
    "        return \" \".join(ai)\n",
    "\n",
    "    def get_chatlog(row):\n",
    "        #return \"Hidden\"\n",
    "        if len(chatlogs[chatlogs['worker_id'] == row['mturk_id']]) == 0:\n",
    "            return\n",
    "        return chatlogs[chatlogs['worker_id'] == row['mturk_id']]['content'].values[0]\n",
    "\n",
    "    def get_human_message_count(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            human = messages[(messages['worker_id'] == row['mturk_id']) & (~pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[0]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "\n",
    "            human = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "\n",
    "        return len([h for h in human if type(h) == str])\n",
    "\n",
    "    def get_ai_message_count(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            ai = messages[(messages['worker_id'] == row['mturk_id']) & (pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:    \n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[1]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "            ai = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "        \n",
    "        return len([h for h in ai if type(h) == str])\n",
    "\n",
    "    def get_human_word_count(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            human = messages[(messages['worker_id'] == row['mturk_id']) & (~pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[0]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "            human = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "        human = [len(h.split(\" \")) for h in human if type(h) == str]\n",
    "        return json.dumps(human)\n",
    "\n",
    "    def get_ai_word_count(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            ai = messages[(messages['worker_id'] == row['mturk_id']) & (pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[1]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "            ai = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "        ai = [len(h.split(\" \")) for h in ai if type(h) == str]\n",
    "        return json.dumps(ai)\n",
    "\n",
    "    def get_avg_human_word_count(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            human = messages[(messages['worker_id'] == row['mturk_id']) & (~pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[0]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "            human = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "        human = [len(h.split(\" \")) for h in human if type(h) == str]\n",
    "\n",
    "        if len(human) == 0:\n",
    "            return -1\n",
    "        \n",
    "        return statistics.mean(human)\n",
    "\n",
    "    def get_avg_ai_word_count(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            ai = messages[(messages['worker_id'] == row['mturk_id']) & (pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[1]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return\n",
    "            ai = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "        ai = [len(h.split(\" \")) for h in ai if type(h) == str]\n",
    "\n",
    "        if len(ai) == 0:\n",
    "            return -1\n",
    "        return statistics.mean(ai)\n",
    "\n",
    "    def get_response_times(row):\n",
    "        msgs = messages[(messages['worker_id'] == row['mturk_id'])]\n",
    "\n",
    "        if (row['condition'] == 'another person' and row['condition_2'] == '0'): # Real person condition\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return json.dumps([])\n",
    "            \n",
    "            msgs = messages[(messages['room_id'] == room_id)]\n",
    "        response_times = []\n",
    "\n",
    "        last_ai_timestamp = None\n",
    "        last_ai_message = None\n",
    "        pair_started = False\n",
    "        last_message_is_human = False\n",
    "        for index, msg in msgs.iterrows():\n",
    "            if not (row['condition'] == 'another person' and row['condition_2'] == '0'):  ## AI Condition\n",
    "                if not pair_started and not pd.isna(msg['user_id']):  # Human sent a message before AI did\n",
    "                    continue\n",
    "                elif pd.isna(msg['user_id']):\n",
    "                    pair_started = True\n",
    "\n",
    "                if pd.isna(msg['user_id']):  # AI sends message\n",
    "                    last_ai_timestamp = datetime.strptime(msg['date_added'], \"%Y-%m-%d %H:%M:%S.%f+00\")\n",
    "                    last_ai_message = msg['content']\n",
    "                    last_message_is_human = False\n",
    "                else:  # Human sends message\n",
    "                    if not last_message_is_human:\n",
    "                        response_time = (datetime.strptime(msg['date_added'], \"%Y-%m-%d %H:%M:%S.%f+00\") -\n",
    "                                     last_ai_timestamp).total_seconds()\n",
    "\n",
    "                        # Also deduct the writing time of AI\n",
    "                        response_time -= len(last_ai_message.split(\" \")) * 0.6  # 0.6 second for each word\n",
    "                        response_times.append(response_time)\n",
    "\n",
    "                    last_message_is_human = True\n",
    "            else:  # Real Person condition\n",
    "                try:\n",
    "                    user = list(set(messages[messages['worker_id'] == row['workerId']]['username']))[0]\n",
    "                except:\n",
    "                    print(\"NA\", row['workerId'])\n",
    "                    return json.dumps([])\n",
    "\n",
    "                if not pair_started and msg['username'] == user:  # Human sent a message before its pair did\n",
    "                    continue\n",
    "\n",
    "                elif msg['username'] != user:\n",
    "                    pair_started = True\n",
    "\n",
    "                if msg['username'] != user: # Pair sends message\n",
    "                    last_ai_timestamp = datetime.strptime(msg['date_added'], \"%Y-%m-%d %H:%M:%S.%f+00\")\n",
    "                    last_ai_message = msg['content']\n",
    "                    last_message_is_human = False\n",
    "                else:  # Human sends message\n",
    "                    if not last_message_is_human:\n",
    "                        response_time = (datetime.strptime(msg['date_added'], \"%Y-%m-%d %H:%M:%S.%f+00\") -\n",
    "                                     last_ai_timestamp).total_seconds()\n",
    "                    \n",
    "                        response_times.append(response_time)\n",
    "\n",
    "                    last_message_is_human = True\n",
    "\n",
    "        return json.dumps(response_times)\n",
    "\n",
    "    def get_avg_response_time(row):\n",
    "        response_times = json.loads(get_response_times(row))\n",
    "\n",
    "        if len(response_times) > 0:\n",
    "            return statistics.mean(response_times)\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    def get_human_compound_sentiments(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            msgs_human = messages[(messages['worker_id'] == row['mturk_id']) & (~pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[0]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return json.dumps([])\n",
    "            \n",
    "            msgs_human = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "\n",
    "        return json.dumps([sia.polarity_scores(message)['compound'] for message in msgs_human if type(message) == str])\n",
    "\n",
    "    def get_ai_compound_sentiments(row):\n",
    "        if not (row['condition'] == 'another person' and row['condition_2'] == '0'):\n",
    "            msgs_ai = messages[(messages['worker_id'] == row['mturk_id']) & (pd.isna(messages['user_id']))][\n",
    "                'content'].values.tolist()\n",
    "        else:\n",
    "            try:\n",
    "                room_list = list(set(messages[messages['worker_id'] == row['workerId']]['room_id']))\n",
    "                room_id = room_list[0]\n",
    "                user = list(set(messages[messages['room_id'] == room_id]['participant_username'].values.tolist()))[1]\n",
    "            except:\n",
    "                print(\"NA\", row['workerId'])\n",
    "                return json.dumps([])\n",
    "            \n",
    "            msgs_ai = messages[(messages['room_id'] == room_id) & (messages['username'] == user)]['content'].values.tolist()\n",
    "\n",
    "        return json.dumps([sia.polarity_scores(message)['compound'] for message in msgs_ai if type(message) == str])\n",
    "\n",
    "    def get_avg_human_compound_sentiment(row):\n",
    "        scores = json.loads(get_human_compound_sentiments(row))\n",
    "        if len(scores) == 0:\n",
    "            return -1\n",
    "        return statistics.mean(scores)\n",
    "\n",
    "    def get_avg_ai_compound_sentiment(row):\n",
    "        scores = json.loads(get_ai_compound_sentiments(row))\n",
    "        if len(scores) == 0:\n",
    "            return -1\n",
    "        return statistics.mean(scores)\n",
    "\n",
    "\n",
    "    df['ResponseTimes'] = df.apply(get_response_times, axis=1)\n",
    "    df['AppendedHumanMessages'] = df.apply(get_appended_human_messages, axis=1)\n",
    "    df['AppendedAIMessages'] = df.apply(get_appended_ai_messages, axis=1)\n",
    "    df['FullChatlog'] = df.apply(get_chatlog, axis=1)\n",
    "    df['HumanMessageCount'] = df.apply(get_human_message_count, axis=1)\n",
    "    df['AIMessageCount'] = df.apply(get_ai_message_count, axis=1)\n",
    "    df['HumanWordCounts'] = df.apply(get_human_word_count, axis=1)\n",
    "    df['AIWordCounts'] = df.apply(get_ai_word_count, axis=1)\n",
    "    df['AvgHumanWordCount'] = df.apply(get_avg_human_word_count, axis=1)\n",
    "    df['AvgAIWordCount'] = df.apply(get_avg_ai_word_count, axis=1)\n",
    "    df['AICompoundSentiments'] = df.apply(get_ai_compound_sentiments, axis=1)\n",
    "    df['HumanCompoundSentiments'] = df.apply(get_human_compound_sentiments, axis=1)\n",
    "    df['AvgAICompoundSentiment'] = df.apply(get_avg_ai_compound_sentiment, axis=1)\n",
    "    df['AvgHumanCompoundSentiment'] = df.apply(get_avg_human_compound_sentiment, axis=1)\n",
    "    df['AvgResponseTime'] = df.apply(get_avg_response_time, axis=1)\n",
    "\n",
    "    df.to_csv('./data/data_with_conversation_stats.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Conversation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Generated File Containing Conversation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_with_conversation_stats.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Below Are For Pairwise Analyses (e.g., sentiment similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## IMPORTS ##############\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import statistics\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as R\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "pandas2ri.activate()\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "utils = importr('utils')\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "utils.install_packages('effsize')\n",
    "effsize = importr('effsize')\n",
    "\n",
    "import csv\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "base = importr('base')\n",
    "stats = importr(\"stats\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Tagset: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## READ THE DATA ##############\n",
    "\n",
    "results = {\"human\": [], \"ai\": [], \"all\": []}\n",
    "\n",
    "sender_type = \"human\" # ai, human, all\n",
    "label_type = \"willing_friend\"  # willing_friend, willing_romantic, wtp\n",
    "\n",
    "# Dataset\n",
    "data_all = df\n",
    "\n",
    "all_messages = {'ai': [], 'human': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## CONSTRUCT LSM INSTANCE ##############\n",
    "import math\n",
    "class LinguisticStyleMatching:\n",
    "    data = None\n",
    "    sia = None\n",
    "    tokenizer = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = data_all\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Return each AI and human message separately, to compare later\n",
    "    def get_paired_messages(self):\n",
    "        data = pd.read_csv('./data/messages.csv')\n",
    "        curr_room = None\n",
    "\n",
    "        all_pairs = {}\n",
    "        curr_message_pairs = {'human-ai': [], 'ai-human': []}\n",
    "        last_message = {'sender_type': '', 'content': ''}\n",
    "\n",
    "        for index, message in data.iterrows():\n",
    "            if curr_room is None:\n",
    "                curr_room = message['room_id']\n",
    "\n",
    "            if curr_room != message['room_id']: # Room changed, save\n",
    "                # Get users of current room\n",
    "                users_list = list(set(data[data['room_id'] == curr_room]['worker_id'].values.tolist()))\n",
    "                for user in users_list:\n",
    "                    # Save the pairs for all users in room\n",
    "                    all_pairs[user] = curr_message_pairs\n",
    "                \n",
    "                curr_room = message['room_id']\n",
    "\n",
    "                # Reset\n",
    "                curr_message_pairs = {'human-ai': [], 'ai-human': []}\n",
    "                last_message = {'sender_type': '', 'content': ''}\n",
    "\n",
    "            # Keep adding to curr_messages\n",
    "            users_list = list(set(data[data['room_id'] == curr_room]['participant_username'].values.tolist()))\n",
    "            if len(users_list) == 0:\n",
    "                continue\n",
    "            \n",
    "            if len(users_list) == 1:\n",
    "                if message['username'] == 'Jessie': # AI message\n",
    "                    if last_message['sender_type'] == 'ai':  # Take only the first message, in case there are consecutive ones\n",
    "                        continue\n",
    "\n",
    "                    if last_message['sender_type'] == 'human':  # Second message, Save the pair\n",
    "                        curr_message_pairs['human-ai'].append([last_message['content'], message['content']])\n",
    "\n",
    "                    last_message = {'sender_type': 'ai', 'content': message['content']}\n",
    "                else:\n",
    "                    if last_message['sender_type'] == 'human':\n",
    "                        continue\n",
    "\n",
    "                    if last_message['sender_type'] == 'ai':  # Not None\n",
    "                        curr_message_pairs['ai-human'].append([last_message['content'], message['content']])\n",
    "\n",
    "                    last_message = {'sender_type': 'human', 'content': message['content']}\n",
    "            else:\n",
    "                user = users_list[1]\n",
    "                if message['username'] == user: # AI message\n",
    "                    if last_message['sender_type'] == 'ai':  # Take only the first message, in case there are consecutive ones\n",
    "                        continue\n",
    "\n",
    "                    if last_message['sender_type'] == 'human':  # Second message, Save the pair\n",
    "                        curr_message_pairs['human-ai'].append([last_message['content'], message['content']])\n",
    "\n",
    "                    last_message = {'sender_type': 'ai', 'content': message['content']}\n",
    "                else:\n",
    "                    if last_message['sender_type'] == 'human':\n",
    "                        continue\n",
    "\n",
    "                    if last_message['sender_type'] == 'ai':  # Not None\n",
    "                        curr_message_pairs['ai-human'].append([last_message['content'], message['content']])\n",
    "\n",
    "                    last_message = {'sender_type': 'human', 'content': message['content']}\n",
    "\n",
    "            if index == data.shape[0] - 1:\n",
    "                # Get users of current room\n",
    "                users_list = list(set(data[data['room_id'] == curr_room]['worker_id'].values.tolist()))\n",
    "                for user in users_list:\n",
    "                    # Save the pairs for all users in room\n",
    "                    all_pairs[user] = curr_message_pairs\n",
    "\n",
    "\n",
    "        return all_pairs\n",
    "\n",
    "\n",
    "    # Normalize the data to interval [0, 1], before calculating similarity\n",
    "    # zi = (xi – min(x)) / (max(x) – min(x))\n",
    "    def normalize_val(self, num):\n",
    "        return (num + 1) / 2\n",
    "\n",
    "\n",
    "    # Calculate average turn-by-turn similarities and add it to self.data\n",
    "    def calculate_turn_similarities(self):\n",
    "\n",
    "        # Create Columns\n",
    "        for sim_type in ['Spacy', 'TurnLength', 'Valence']:\n",
    "            self.data['HumanAISim{}'.format(sim_type)] = None\n",
    "            self.data['AIHumanSim{}'.format(sim_type)] = None\n",
    "            self.data['AvgSim{}'.format(sim_type)] = None\n",
    "\n",
    "        paired_messages = self.get_paired_messages()\n",
    "\n",
    "        for worker_id, pairs in paired_messages.items():  # All subjects\n",
    "            sim = {'Spacy': {}, 'TurnLength': {}, 'Valence': {}}\n",
    "            for pair_type, messages in pairs.items():  # Iterate Both Pair Types, i.e., Human-AI or AI-Human\n",
    "                similarities = {'Spacy': [], 'TurnLength': [], 'Valence': []}\n",
    "                for pair in messages:  # Iterate each pair\n",
    "                    try:\n",
    "                        #### Calculate Spacy Similarity ####\n",
    "                        spacy_similarity = nlp(str(pair[0])).similarity(nlp(str(pair[1])))\n",
    "                        similarities['Spacy'].append(100 * spacy_similarity)\n",
    "\n",
    "                        #### Calculate Turn Length Similarity ####\n",
    "                        turn_similarity = self.calculate_sim(len(self.tokenizer.tokenize(str(pair[0]))), len(self.tokenizer.tokenize(str(pair[1]))))\n",
    "                        similarities['TurnLength'].append(turn_similarity)\n",
    "\n",
    "                        #### Calculate Valence Similarity ####\n",
    "                        # Normalize the data to interval [0, 1], before calculating similarity\n",
    "                        polarity_1 = self.normalize_val(self.sia.polarity_scores(str(pair[0]))['compound'])\n",
    "                        polarity_2 = self.normalize_val(self.sia.polarity_scores(str(pair[1]))['compound'])\n",
    "                        valence_similarity = self.calculate_sim(polarity_1, polarity_2)\n",
    "                        similarities['Valence'].append(valence_similarity)\n",
    "\n",
    "                        #print(\" -- Spacy Similarity: \", spacy_similarity, \" -- Turn Length Similarity: \", turn_similarity, \" -- Valence Similarity: \", valence_similarity)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                # Calculate mean and std. deviation between pairs\n",
    "                for sim_type in similarities.keys():\n",
    "                    sim[sim_type][pair_type] = {'mean': statistics.mean(similarities[sim_type]) if len(similarities[sim_type]) > 1 else -1, 'sd': statistics.stdev(similarities[sim_type]) if len(similarities[sim_type]) > 1 else -1}\n",
    "\n",
    "            for sim_type in sim.keys():\n",
    "                self.data.loc[self.data['mturk_id'] == worker_id, 'HumanAISim{}'.format(sim_type)] = sim[sim_type]['human-ai']['mean']\n",
    "                self.data.loc[self.data['mturk_id'] == worker_id, 'AIHumanSim{}'.format(sim_type)] = sim[sim_type]['ai-human']['mean']\n",
    "                self.data.loc[self.data['mturk_id'] == worker_id, 'AvgSim{}'.format(sim_type)] = statistics.mean([sim[sim_type]['ai-human']['mean'], sim[sim_type]['human-ai']['mean']])\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize_raw_data(self):\n",
    "        def tokenize(conversation):\n",
    "            if type(conversation) == float and math.isnan(conversation):\n",
    "                return \"\"\n",
    "            return nltk.pos_tag(nltk.word_tokenize(conversation))\n",
    "\n",
    "        self.data.loc[:, 'HumanTags'] = self.data.loc[:, 'AppendedHumanMessages'].apply(tokenize)\n",
    "        self.data.loc[:, 'AITags'] = self.data.loc[:, 'AppendedAIMessages'].apply(tokenize)\n",
    "\n",
    "    def count_tokens(self):\n",
    "        def count_tokens(tokens):\n",
    "            return dict(Counter(token[1] for token in tokens))\n",
    "\n",
    "        self.data.loc[:, 'HumanTokenCounts'] = self.data.loc[:, 'HumanTags'].apply(count_tokens)\n",
    "        self.data.loc[:, 'AITokenCounts'] = self.data.loc[:, 'AITags'].apply(count_tokens)\n",
    "\n",
    "    def calculate_style_matching(self):\n",
    "        def calculate(human_tags, ai_tags):\n",
    "            # Ignore Punctuations\n",
    "            if \".\" in human_tags.keys():\n",
    "                human_tags.pop(\".\")\n",
    "\n",
    "            if \".\" in ai_tags.keys():\n",
    "                ai_tags.pop(\".\")\n",
    "\n",
    "            human_total_tags = sum(human_tags.values())\n",
    "            ai_total_tags = sum(ai_tags.values())\n",
    "            matchings = {}\n",
    "            for k in self.get_all_token_names(): # Calculate only function words\n",
    "                try:\n",
    "                    human_prop = human_tags.get(k, 0) / human_total_tags\n",
    "                    ai_prop = ai_tags.get(k, 0) / ai_total_tags\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "                matchings[k] = self.calculate_sim(human_prop, ai_prop)\n",
    "\n",
    "            return matchings\n",
    "\n",
    "        self.data['StyleMatching'] = self.data.apply(lambda x: calculate(x.HumanTokenCounts, x.AITokenCounts), axis=1)\n",
    "\n",
    "    def get_all_token_names(self):\n",
    "        return ['CC', 'DT', 'PRP', 'PRP$', 'WP', 'IN']\n",
    "\n",
    "    def get_all_tokens(self):\n",
    "        tokens = self.get_all_token_names()\n",
    "\n",
    "        def get_token(row): # Return tokens\n",
    "            return { token: row.get(token, 0) for token in tokens }\n",
    "\n",
    "        return self.data.loc[:, 'StyleMatching'].apply(get_token)\n",
    "\n",
    "    # Mean of all common tokens\n",
    "    def get_lsms(self):\n",
    "        tokens = self.get_all_tokens()\n",
    "        token_names = self.get_all_token_names()\n",
    "\n",
    "        def get_avg(row):\n",
    "            avg = []\n",
    "            for token in token_names:\n",
    "                try:\n",
    "                    avg.append(row[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            if len(avg) == 0:\n",
    "                return -1\n",
    "            return statistics.mean(avg)\n",
    "\n",
    "        self.data['AvgSimStyleMatching'] = self.data.loc[:, 'StyleMatching'].apply(get_avg)\n",
    "        return self.data['AvgSimStyleMatching']\n",
    "\n",
    "\n",
    "    # Mean LSM among all conversations\n",
    "    def get_mean_lsm(self):\n",
    "        tokens = self.get_all_token_names()\n",
    "        token_avgs = { token : [] for token in tokens }\n",
    "        for index, row in self.data.iterrows():\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    token_avgs[token].append(row['StyleMatching'][token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "        return { k: {'Mean': sum(v) / len(v), 'Std. Dev': statistics.stdev(v)} for k, v in token_avgs.items() }\n",
    "\n",
    "    # Returns the similarity between two numbers, i.e., similarity = 1 − (|num1 − num2|/(num1 + num2))\n",
    "    def calculate_sim(self, num1, num2):\n",
    "        if num1 + num2 == 0:\n",
    "            return 100\n",
    "        return 100 * (1 - (abs(num1 - num2) / (num1 + num2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lsm = LinguisticStyleMatching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############### Setup For Linguistic Style Matching ###############\n",
    "\n",
    "lsm.tokenize_raw_data()\n",
    "lsm.count_tokens()\n",
    "lsm.calculate_style_matching()\n",
    "lsm.get_mean_lsm()\n",
    "lsm.get_all_tokens()\n",
    "lsms = lsm.get_lsms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm.get_mean_lsm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm.calculate_turn_similarities()  # Calculate turn-by-turn similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm.data[['workerId', 'AvgSimSpacy', 'AvgSimTurnLength', 'AvgSimValence', 'AvgSimStyleMatching']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge with our original dataset\n",
    "df = df.merge(lsm.data[['workerId', 'AvgSimSpacy', 'AvgSimTurnLength', 'AvgSimValence', 'AvgSimStyleMatching']], on='workerId')\n",
    "df.to_csv(\"./data/final_data.csv\")  ### Save similarity data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ba5ede13f0387f51f50b72b517e398c81deb77e749ad9440ae5ea50ce01832d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
