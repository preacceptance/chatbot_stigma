{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "\n",
    "try:\n",
    "    import tomotopy as tp\n",
    "except Exception as e:\n",
    "    print(\"Error importing tomotopy. Please install it using 'pip install tomotopy'\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def read_json(fname):\n",
    "    with open(fname) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def run_topic_models(df,  msg_sender, condition):\n",
    "    try:\n",
    "        # Load the preprocessed corpus\n",
    "        corpus = tp.utils.Corpus.load('./data/corpus/mtype={}_cond={}.cps'.format( msg_sender, condition))\n",
    "    except:\n",
    "        stemmer = nltk.SnowballStemmer('english').stem\n",
    "        english_stops = set(stemmer(w) for w in stopwords.words('english'))\n",
    "        pat = re.compile('^[a-z]{2,}$')\n",
    "        corpus = tp.utils.Corpus(\n",
    "            tokenizer=tp.utils.SimpleTokenizer(stemmer),\n",
    "            stopwords=lambda x: x in english_stops or not pat.match(x)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            if condition == \"all\":\n",
    "                conversations = df\n",
    "            else:\n",
    "                conversations = df[df['agent_cond'] == condition]\n",
    "\n",
    "            if  msg_sender == \"ai\":\n",
    "                conversations = conversations['AppendedAIMessages']\n",
    "            elif  msg_sender == \"human\":\n",
    "                conversations = conversations['AppendedHumanMessages']\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "                return\n",
    "\n",
    "        except:\n",
    "            return\n",
    "\n",
    "        corpus.process(conversation.lower() for conversation in conversations.values)\n",
    "        corpus.save('./data/corpus/mtype={}_cond={}.cps'.format( msg_sender, condition))\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    \n",
    "    mdl = tp.LDAModel(min_df=5, rm_top=5, k=15, corpus=corpus, seed=0)\n",
    "    mdl.train(0)\n",
    "\n",
    "    write_file = open(\"./topicmodel_logs/logs_mtype={}_condition={}.txt\".format( msg_sender, condition), \"w+\")\n",
    "\n",
    "    print('Num docs:{}, Num Vocabs:{}, Total Words:{}'.format(\n",
    "        len(mdl.docs), len(mdl.used_vocabs), mdl.num_words\n",
    "    ), file=write_file)\n",
    "    print('Removed Top words: ', *mdl.removed_top_words, file=write_file)\n",
    "\n",
    "    # Let's train the model\n",
    "    for i in range(0, 1000, 20):\n",
    "        print('Iteration: {:04}, LL per word: {:.4}'.format(i, mdl.ll_per_word), file=write_file)\n",
    "        mdl.train(100)\n",
    "    print('Iteration: {:04}, LL per word: {:.4}'.format(1000, mdl.ll_per_word), file=write_file)\n",
    "\n",
    "    mdl.summary()\n",
    "\n",
    "    topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
    "    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
    "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "    doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
    "    vocab = list(mdl.used_vocabs)\n",
    "    term_frequency = mdl.used_vocab_freq\n",
    "\n",
    "    if len(vocab) < 10:\n",
    "        print(\"Not enough conversations. Terminating...\", file=write_file)\n",
    "        return\n",
    "\n",
    "    prepared_data = pyLDAvis.prepare(\n",
    "        topic_term_dists,\n",
    "        doc_topic_dists,\n",
    "        doc_lengths,\n",
    "        vocab,\n",
    "        term_frequency,\n",
    "        sort_topics=False,\n",
    "        mds='mmds'\n",
    "    )\n",
    "\n",
    "    fname = './model_visuals/msg_sender={}_condition={}.html'.format( msg_sender, condition)\n",
    "\n",
    "    pyLDAvis.save_html(prepared_data, fname)\n",
    "    # return mdl_words  # Return top 10 words of each topic\n",
    "\n",
    "\n",
    "def process():\n",
    "    df = pd.read_csv('./data/clean_combined_data.csv')\n",
    "    df = df.loc[df['interact_cond'] == 'yes']\n",
    "\n",
    "\n",
    "    chatlogs = pd.read_csv('./data/chatlogs.csv')\n",
    "    messages = pd.read_csv('./data/messages.csv')\n",
    "\n",
    "    data = {\n",
    "        'human': {'human_cond_processed': [], 'ai_cond_processed': [], 'human_cond_wait': [],\n",
    "                  'ai_cond_wait': [], 'all_len': [], 'all_len_d': {}, 'human_cond_formatted': {},\n",
    "                  'all_formatted': {}, 'ai_cond_formatted': {}},\n",
    "        'ai': {'human_cond_processed': [], 'ai_cond_processed': [], 'human_cond_wait': [],\n",
    "               'ai_cond_wait': [], 'all_len': [], 'all_len_d': {}, 'human_cond_formatted': {},\n",
    "               'all_formatted': {}, 'ai_cond_formatted': {}}}\n",
    "\n",
    "    def get_appended_human_messages(row):\n",
    "        human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))]['content'].values.tolist()\n",
    "        human = [h for h in human if type(h) == str]\n",
    "\n",
    "        #return \"Hidden\"\n",
    "        return \" \".join(human)\n",
    "\n",
    "    def get_appended_ai_messages(row):\n",
    "        ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))]['content'].values.tolist()\n",
    "        ai = [h for h in ai if type(h) == str]\n",
    "\n",
    "        #return \"Hidden\"\n",
    "        return \" \".join(ai)\n",
    "\n",
    "    def get_chatlog(row):\n",
    "        #return \"Hidden\"\n",
    "        return chatlogs[chatlogs['worker_id'] == row['worker_id']]['content'].values[0]\n",
    "\n",
    "    def get_human_message_count(row):\n",
    "        human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        return len([h for h in human if type(h) == str])\n",
    "\n",
    "    def get_ai_message_count(row):\n",
    "        ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        return len([h for h in ai if type(h) == str])\n",
    "\n",
    "    def get_human_word_count(row):\n",
    "        human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        human = [len(h.split(\" \")) for h in human if type(h) == str]\n",
    "        return json.dumps(human)\n",
    "        \n",
    "\n",
    "    def get_ai_word_count(row):\n",
    "        ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        ai = [len(h.split(\" \")) for h in ai if type(h) == str]\n",
    "        return json.dumps(ai)\n",
    "    \n",
    "    def get_sum_human_char_count(row):\n",
    "        human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        human = sum([len(h) for h in human if type(h) == str])\n",
    "        return json.dumps(human)\n",
    "        \n",
    "\n",
    "    def get_sum_ai_char_count(row):\n",
    "        ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        ai = sum([len(h) for h in ai if type(h) == str])\n",
    "        return json.dumps(ai)\n",
    "\n",
    "    def get_avg_human_word_count(row):\n",
    "        human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        human = [len(h.split(\" \")) for h in human if type(h) == str]\n",
    "        return statistics.mean(human)\n",
    "\n",
    "    def get_avg_ai_word_count(row):\n",
    "        ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        ai = [len(h.split(\" \")) for h in ai if type(h) == str]\n",
    "        return statistics.mean(ai)\n",
    "\n",
    "    def get_response_times(row):\n",
    "        msgs = messages[(messages['worker_id'] == row['worker_id'])]\n",
    "        response_times = []\n",
    "\n",
    "        last_ai_timestamp = None\n",
    "        last_ai_message = None\n",
    "        ai_started = False\n",
    "        last_message_is_human = False\n",
    "        for index, msg in msgs.iterrows():\n",
    "            if not ai_started and not pd.isna(msg['user_id']):  # Human sent a message before AI did\n",
    "                continue\n",
    "            elif pd.isna(msg['user_id']):\n",
    "                ai_started = True\n",
    "\n",
    "            if pd.isna(msg['user_id']):  # AI sends message\n",
    "                last_ai_timestamp = datetime.strptime(msg['date_added'], \"%Y-%m-%d %H:%M:%S.%f+00\")\n",
    "                last_ai_message = msg['content']\n",
    "                last_message_is_human = False\n",
    "            else:  # Human sends message\n",
    "                if not last_message_is_human:\n",
    "                    # Removing the time elapsed for 'Writing...' notification, as well as the estimated typing/reading time\n",
    "                    response_time = (datetime.strptime(msg['date_added'], \"%Y-%m-%d %H:%M:%S.%f+00\") -\n",
    "                                     last_ai_timestamp).total_seconds()\n",
    "                    \n",
    "                    response_time -= len(last_ai_message.split(\" \")) * 0.6  # 0.6 second for each word\n",
    "                    response_times.append(response_time)\n",
    "\n",
    "                last_message_is_human = True\n",
    "\n",
    "        return json.dumps(response_times)\n",
    "\n",
    "    def get_avg_response_time(row):\n",
    "        return statistics.mean(json.loads(get_response_times(row)))\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    def get_human_compound_sentiments(row):\n",
    "        msgs_human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        return json.dumps([sia.polarity_scores(message)['compound'] for message in msgs_human if type(message) == str])\n",
    "\n",
    "    def get_ai_compound_sentiments(row):\n",
    "        msgs_ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        return json.dumps([sia.polarity_scores(message)['compound'] for message in msgs_ai if type(message) == str])\n",
    "\n",
    "    def get_avg_human_compound_sentiment(row):\n",
    "        msgs_human = messages[(messages['worker_id'] == row['worker_id']) & (~pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        return statistics.mean([sia.polarity_scores(message)['compound'] for message in msgs_human if type(message) == str])\n",
    "\n",
    "    def get_avg_ai_compound_sentiment(row):\n",
    "        msgs_ai = messages[(messages['worker_id'] == row['worker_id']) & (pd.isna(messages['user_id']))][\n",
    "            'content'].values.tolist()\n",
    "        return statistics.mean([sia.polarity_scores(message)['compound'] for message in msgs_ai if type(message) == str])\n",
    "\n",
    "    df['AppendedHumanMessages'] = df.apply(get_appended_human_messages, axis=1)\n",
    "    df['AppendedAIMessages'] = df.apply(get_appended_ai_messages, axis=1)\n",
    "    df['FullChatlog'] = df.apply(get_chatlog, axis=1)\n",
    "    df['HumanMessageCount'] = df.apply(get_human_message_count, axis=1)\n",
    "    df['AIMessageCount'] = df.apply(get_ai_message_count, axis=1)\n",
    "    df['HumanWordCounts'] = df.apply(get_human_word_count, axis=1)\n",
    "    df['AIWordCounts'] = df.apply(get_ai_word_count, axis=1)\n",
    "    df['AvgHumanWordCount'] = df.apply(get_avg_human_word_count, axis=1)\n",
    "    df['AvgAIWordCount'] = df.apply(get_avg_ai_word_count, axis=1)\n",
    "    df['SumHumanCharCount'] = df.apply(get_sum_human_char_count, axis=1)\n",
    "    df['SumAICharCount'] = df.apply(get_sum_ai_char_count, axis=1)\n",
    "    df['ResponseTimes'] = df.apply(get_response_times, axis=1)\n",
    "    df['AICompoundSentiments'] = df.apply(get_ai_compound_sentiments, axis=1)\n",
    "    df['HumanCompoundSentiments'] = df.apply(get_human_compound_sentiments, axis=1)\n",
    "    df['AvgAICompoundSentiment'] = df.apply(get_avg_ai_compound_sentiment, axis=1)\n",
    "    df['AvgHumanCompoundSentiment'] = df.apply(get_avg_human_compound_sentiment, axis=1)\n",
    "    df['AvgResponseTime'] = df.apply(get_avg_response_time, axis=1)\n",
    "\n",
    "    df.to_csv('./data/clean_detailed_int_data.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Conversation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Generated File Containing Conversation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/clean_detailed_int_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for msg_sender in ['human', 'ai']:\n",
    "    for condition in ['human', 'ai']:\n",
    "        run_topic_models(df, msg_sender=msg_sender, condition=condition)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Below Are For Pairwise Analyses (e.g., sentiment similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 'python -m spacy download en_core_web_lg' to download the large English model for Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## IMPORTS ##############\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import statistics\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as R\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "utils = importr('utils')\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "utils.install_packages('effsize')\n",
    "effsize = importr('effsize')\n",
    "\n",
    "import csv\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import spacy.cli\n",
    "\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "base = importr('base')\n",
    "stats = importr(\"stats\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tagset: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## READ THE DATA ##############\n",
    "\n",
    "results = {\"human\": [], \"ai\": [], \"all\": []}\n",
    "\n",
    "sender_type = \"human\" # ai, human, all\n",
    "label_type = \"willing_friend\"  # willing_friend, willing_romantic, wtp\n",
    "\n",
    "# Dataset\n",
    "data_all = df[['FullChatlog', 'AppendedHumanMessages', 'AppendedAIMessages', 'willing_friend', 'willing_romantic', 'wtp', 'agent_cond', 'worker_id', 'belief']]\n",
    "\n",
    "all_messages = {'ai': [], 'human': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## CONSTRUCT LSM INSTANCE ##############\n",
    "\n",
    "class LinguisticStyleMatching:\n",
    "    data = None\n",
    "    sia = None\n",
    "    tokenizer = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = data_all[['AppendedHumanMessages', 'AppendedAIMessages', 'willing_friend', 'willing_romantic', 'wtp', 'agent_cond', 'worker_id', 'belief']]\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Return each AI and human message separately, to compare later\n",
    "    def get_paired_messages(self):\n",
    "        data = pd.read_csv('./data/messages.csv')\n",
    "        curr_worker = None\n",
    "\n",
    "        all_pairs = {}\n",
    "        curr_message_pairs = {'human-ai': [], 'ai-human': []}\n",
    "        last_message = {'sender_type': '', 'content': ''}\n",
    "\n",
    "        for index, message in data.iterrows():\n",
    "            curr_worker = message['worker_id'] if curr_worker is None else curr_worker  # First Iteration\n",
    "\n",
    "            if curr_worker != message['worker_id']: # Room changed, save\n",
    "                all_pairs[curr_worker] = curr_message_pairs\n",
    "                curr_worker = message['worker_id']\n",
    "\n",
    "                # Reset\n",
    "                curr_message_pairs = {'human-ai': [], 'ai-human': []}\n",
    "                last_message = {'sender_type': '', 'content': ''}\n",
    "\n",
    "            # Keep adding to curr_messages\n",
    "            if message['username'] == 'Jessie': # AI message\n",
    "                if last_message['sender_type'] == 'ai':  # Take only the first message, in case there are consecutive ones\n",
    "                    continue\n",
    "\n",
    "                if last_message['sender_type'] == 'human':  # Second message, Save the pair\n",
    "                    curr_message_pairs['human-ai'].append([last_message['content'], message['content']])\n",
    "\n",
    "                last_message = {'sender_type': 'ai', 'content': message['content']}\n",
    "            else:\n",
    "                if last_message['sender_type'] == 'human':\n",
    "                    continue\n",
    "\n",
    "                if last_message['sender_type'] == 'ai':  # Not None\n",
    "                    curr_message_pairs['ai-human'].append([last_message['content'], message['content']])\n",
    "\n",
    "                last_message = {'sender_type': 'human', 'content': message['content']}\n",
    "\n",
    "            if index == data.shape[0] - 1:\n",
    "                all_pairs[curr_worker] = curr_message_pairs\n",
    "\n",
    "\n",
    "        return all_pairs\n",
    "\n",
    "\n",
    "    # Normalize the data to interval [0, 1], before calculating similarity\n",
    "    # zi = (xi – min(x)) / (max(x) – min(x))\n",
    "    def normalize_val(self, num):\n",
    "        return (num + 1) / 2\n",
    "\n",
    "\n",
    "    # Calculate average turn-by-turn similarities and add it to self.data\n",
    "    def calculate_turn_similarities(self):\n",
    "\n",
    "        # Create Columns\n",
    "        for sim_type in ['Spacy', 'TurnLength', 'Valence']:\n",
    "            self.data['HumanAISim{}'.format(sim_type)] = None\n",
    "            self.data['AIHumanSim{}'.format(sim_type)] = None\n",
    "            self.data['AvgSim{}'.format(sim_type)] = None\n",
    "\n",
    "        paired_messages = self.get_paired_messages()\n",
    "\n",
    "        for worker_id, pairs in paired_messages.items():  # All subjects\n",
    "            sim = {'Spacy': {}, 'TurnLength': {}, 'Valence': {}}\n",
    "            for pair_type, messages in pairs.items():  # Iterate Both Pair Types, i.e., Human-AI or AI-Human\n",
    "                similarities = {'Spacy': [], 'TurnLength': [], 'Valence': []}\n",
    "                for pair in messages:  # Iterate each pair\n",
    "                    try:\n",
    "                        #### Calculate Spacy Similarity ####\n",
    "                        spacy_similarity = nlp(str(pair[0])).similarity(nlp(str(pair[1])))\n",
    "                        similarities['Spacy'].append(100 * spacy_similarity)\n",
    "\n",
    "                        #### Calculate Turn Length Similarity ####\n",
    "                        turn_similarity = self.calculate_sim(len(self.tokenizer.tokenize(str(pair[0]))), len(self.tokenizer.tokenize(str(pair[1]))))\n",
    "                        similarities['TurnLength'].append(turn_similarity)\n",
    "\n",
    "                        #### Calculate Valence Similarity ####\n",
    "                        # Normalize the data to interval [0, 1], before calculating similarity\n",
    "                        polarity_1 = self.normalize_val(self.sia.polarity_scores(str(pair[0]))['compound'])\n",
    "                        polarity_2 = self.normalize_val(self.sia.polarity_scores(str(pair[1]))['compound'])\n",
    "                        valence_similarity = self.calculate_sim(polarity_1, polarity_2)\n",
    "                        similarities['Valence'].append(valence_similarity)\n",
    "\n",
    "                        #print(\" -- Spacy Similarity: \", spacy_similarity, \" -- Turn Length Similarity: \", turn_similarity, \" -- Valence Similarity: \", valence_similarity)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                # Calculate mean and std. deviation between pairs\n",
    "                for sim_type in similarities.keys():\n",
    "                    sim[sim_type][pair_type] = {'mean': statistics.mean(similarities[sim_type]), 'sd': statistics.stdev(similarities[sim_type])}\n",
    "\n",
    "            for sim_type in sim.keys():\n",
    "                self.data.loc[self.data['worker_id'] == worker_id, 'HumanAISim{}'.format(sim_type)] = sim[sim_type]['human-ai']['mean']\n",
    "                self.data.loc[self.data['worker_id'] == worker_id, 'AIHumanSim{}'.format(sim_type)] = sim[sim_type]['ai-human']['mean']\n",
    "                self.data.loc[self.data['worker_id'] == worker_id, 'AvgSim{}'.format(sim_type)] = statistics.mean([sim[sim_type]['ai-human']['mean'], sim[sim_type]['human-ai']['mean']])\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize_raw_data(self):\n",
    "        def tokenize(conversation):\n",
    "            return nltk.pos_tag(nltk.word_tokenize(conversation))\n",
    "\n",
    "        self.data.loc[:, 'HumanTags'] = self.data.loc[:, 'AppendedHumanMessages'].apply(tokenize)\n",
    "        self.data.loc[:, 'AITags'] = self.data.loc[:, 'AppendedAIMessages'].apply(tokenize)\n",
    "\n",
    "    def count_tokens(self):\n",
    "        def count_tokens(tokens):\n",
    "            return dict(Counter(token[1] for token in tokens))\n",
    "\n",
    "        self.data.loc[:, 'HumanTokenCounts'] = self.data.loc[:, 'HumanTags'].apply(count_tokens)\n",
    "        self.data.loc[:, 'AITokenCounts'] = self.data.loc[:, 'AITags'].apply(count_tokens)\n",
    "\n",
    "    def calculate_style_matching(self):\n",
    "        def calculate(human_tags, ai_tags):\n",
    "            # Ignore Punctuations\n",
    "            if \".\" in human_tags.keys():\n",
    "                human_tags.pop(\".\")\n",
    "\n",
    "            if \".\" in ai_tags.keys():\n",
    "                ai_tags.pop(\".\")\n",
    "\n",
    "            human_total_tags = sum(human_tags.values())\n",
    "            ai_total_tags = sum(ai_tags.values())\n",
    "            matchings = {}\n",
    "            for k in self.get_all_token_names(): # Calculate only function words\n",
    "                human_prop = human_tags.get(k, 0) / human_total_tags\n",
    "                ai_prop = ai_tags.get(k, 0) / ai_total_tags\n",
    "                matchings[k] = self.calculate_sim(human_prop, ai_prop)\n",
    "\n",
    "            return matchings\n",
    "\n",
    "        self.data['StyleMatching'] = self.data.apply(lambda x: calculate(x.HumanTokenCounts, x.AITokenCounts), axis=1)\n",
    "\n",
    "    def get_all_token_names(self):\n",
    "        return ['CC', 'DT', 'PRP', 'PRP$', 'WP', 'IN']\n",
    "\n",
    "    def get_all_tokens(self):\n",
    "        tokens = self.get_all_token_names()\n",
    "\n",
    "        def get_token(row): # Return tokens\n",
    "            return { token: row.get(token, 0) for token in tokens }\n",
    "\n",
    "        return self.data.loc[:, 'StyleMatching'].apply(get_token)\n",
    "\n",
    "    # Mean of all common tokens\n",
    "    def get_lsms(self):\n",
    "        tokens = self.get_all_tokens()\n",
    "        token_names = self.get_all_token_names()\n",
    "\n",
    "        def get_avg(row):\n",
    "            avg = []\n",
    "            for token in token_names:\n",
    "                avg.append(row[token])\n",
    "\n",
    "            return statistics.mean(avg)\n",
    "\n",
    "        self.data['AvgSimStyleMatching'] = self.data.loc[:, 'StyleMatching'].apply(get_avg)\n",
    "        return self.data['AvgSimStyleMatching']\n",
    "\n",
    "\n",
    "    # Mean LSM among all conversations\n",
    "    def get_mean_lsm(self):\n",
    "        tokens = self.get_all_token_names()\n",
    "        token_avgs = { token : [] for token in tokens }\n",
    "        for index, row in self.data.iterrows():\n",
    "            for token in tokens:\n",
    "                token_avgs[token].append(row['StyleMatching'][token])\n",
    "\n",
    "        return { k: {'Mean': sum(v) / len(v), 'Std. Dev': statistics.stdev(v)} for k, v in token_avgs.items() }\n",
    "\n",
    "    # Returns the similarity between two numbers, i.e., similarity = 1 − (|num1 − num2|/(num1 + num2))\n",
    "    def calculate_sim(self, num1, num2):\n",
    "        if num1 + num2 == 0:\n",
    "            return 100\n",
    "        return 100 * (1 - (abs(num1 - num2) / (num1 + num2)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup For Style Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm = LinguisticStyleMatching()\n",
    "lsm.tokenize_raw_data()\n",
    "lsm.count_tokens()\n",
    "lsm.calculate_style_matching()\n",
    "lsm.get_mean_lsm()\n",
    "lsm.get_all_tokens()\n",
    "lsms = lsm.get_lsms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm.get_mean_lsm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "lsm.calculate_turn_similarities()  # Calculate turn-by-turn similarities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION ANALYSIS ON LSM TOKEN & TURN LENGTH & SENTIMENT & SEMANTIC SIMILARITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfn = pd.DataFrame({'lsm': lsm.data['AvgSimStyleMatching'],\n",
    "                    'semantic_sim': pd.to_numeric(lsm.data['AvgSimSpacy']),\n",
    "                    'sentiment_sim': pd.to_numeric(lsm.data['AvgSimValence']),\n",
    "                    'turn_length_sim': pd.to_numeric(lsm.data['AvgSimTurnLength']),\n",
    "\n",
    "                    'message_count': df['HumanMessageCount'],\n",
    "                    'avg_word_count': df['AvgHumanWordCount'],\n",
    "                    'avg_response_time': df['AvgResponseTime'],\n",
    "                    'avg_sentiment': df['AvgHumanCompoundSentiment'],\n",
    "\n",
    "                    'willing_friend': lsm.data['willing_friend'],\n",
    "                    'willing_romantic': lsm.data['willing_romantic'],\n",
    "                    'wtp': lsm.data['wtp'], 'agent_cond': lsm.data['agent_cond']})\n",
    "\n",
    "df_human = dfn[dfn['agent_cond'] == 'human']\n",
    "df_ai = dfn[dfn['agent_cond'] == 'ai'] # AI Condition\n",
    "\n",
    "dfs = {'human': df_human, 'ai': df_ai, 'all': dfn}\n",
    "for condition in ['all']: # 'human', 'ai'\n",
    "    for dv in ['willing_friend', 'willing_romantic', 'wtp']:\n",
    "        for similarity_type in ['lsm', 'semantic_sim', 'sentiment_sim', 'turn_length_sim', 'message_count', 'avg_word_count', 'avg_response_time', 'avg_sentiment']:\n",
    "            print(\"******* {} Condition -- Does {} predict {}? *******\".format(condition, similarity_type, dv))\n",
    "            lm = stats.lm('{} ~ {}'.format(dv, similarity_type), data=dfs[condition]) # For logistic regression: R.r.glm\n",
    "            print(str(base.summary(lm)).split('Coefficients:')[1])\n",
    "\n",
    "            print(\"M: \", dfs[condition]['lsm'].mean())\n",
    "            print(\"SD: \", dfs[condition]['lsm'].std())\n",
    "            print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm.data[['worker_id', 'AvgSimSpacy', 'AvgSimTurnLength', 'AvgSimValence', 'AvgSimStyleMatching']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge with our original dataset\n",
    "df = df.merge(lsm.data[['worker_id', 'AvgSimSpacy', 'AvgSimTurnLength', 'AvgSimValence', 'AvgSimStyleMatching']], on='worker_id')\n",
    "df.to_csv(\"./data/clean_detailed_int_data.csv\")  ### Save similarity data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lsm_df = pd.DataFrame({'AvgSimSpacy': pd.to_numeric(lsm.data['AvgSimSpacy']),\n",
    "                   'HumanAISimSpacy': pd.to_numeric(lsm.data['HumanAISimSpacy']), \n",
    "                   'AIHumanSimSpacy': pd.to_numeric(lsm.data['AIHumanSimSpacy']), \n",
    "\n",
    "                   'AvgSimValence': pd.to_numeric(lsm.data['AvgSimValence']), \n",
    "                   'HumanAISimValence': pd.to_numeric(lsm.data['HumanAISimValence']), \n",
    "                   'AIHumanSimValence': pd.to_numeric(lsm.data['AIHumanSimValence']), \n",
    "\n",
    "                   'AvgSimTurnLength': pd.to_numeric(lsm.data['AvgSimTurnLength']), \n",
    "                   'HumanAISimTurnLength': pd.to_numeric(lsm.data['HumanAISimTurnLength']), \n",
    "                   'AIHumanSimTurnLength': pd.to_numeric(lsm.data['AIHumanSimTurnLength']),\n",
    "\n",
    "                   'StyleMatching': pd.to_numeric(lsm.data['AvgSimStyleMatching']),\n",
    "                   \n",
    "                   'willing_friend': lsm.data['willing_friend'],\n",
    "                   'willing_romantic': lsm.data['willing_romantic'],\n",
    "                   'wtp': lsm.data['wtp'], 'agent_cond': lsm.data['agent_cond'],\n",
    "                   'worker_id': lsm.data['worker_id'], 'belief': lsm.data['belief']})\n",
    "                          \n",
    "df_human = lsm_df[lsm_df['agent_cond'] == 'human'] # Human Condition\n",
    "df_human = df_human[df_human['belief'] == 1]\n",
    "df_ai = lsm_df[lsm_df['agent_cond'] == 'ai'] # AI Condition\n",
    "\n",
    "dfs = {'human': df_human, 'ai': df_ai, 'all': df}\n",
    "for compare_with in ['human']:\n",
    "    for pair_type in ['Avg']:\n",
    "        for similarity_type in ['StyleMatching', 'Spacy', 'TurnLength', 'Valence']:\n",
    "            print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-* Pair Type: {} - Human vs. AI Condition: {} *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\".format(pair_type, similarity_type))\n",
    "            vt = stats.var_test(R.FloatVector(dfs[compare_with]['{}Sim{}'.format(pair_type, similarity_type) if similarity_type != 'StyleMatching' else 'StyleMatching']),\n",
    "            R.FloatVector(dfs['ai']['{}Sim{}'.format(pair_type, similarity_type) if similarity_type != 'StyleMatching' else 'StyleMatching']))\n",
    "\n",
    "\n",
    "            tt = stats.t_test(R.FloatVector(dfs[compare_with]['{}Sim{}'.format(pair_type, similarity_type) if similarity_type != 'StyleMatching' else 'StyleMatching']),\n",
    "            R.FloatVector(dfs['ai']['{}Sim{}'.format(pair_type, similarity_type) if similarity_type != 'StyleMatching' else 'StyleMatching']),\n",
    "                **{'var.equal': float(vt.rx('p.value')[0][0])>0.05, 'paired': False})\n",
    "\n",
    "            print(tt)\n",
    "\n",
    "            sig = \"None\"\n",
    "            if tt.rx('p.value')[0][0] < 0.001:\n",
    "                sig = \"***\"\n",
    "            elif tt.rx('p.value')[0][0] < 0.01:\n",
    "                sig = \"**\"\n",
    "            elif tt.rx('p.value')[0][0] < 0.05:\n",
    "                sig = \"*\"\n",
    "            elif tt.rx('p.value')[0][0] < 0.1:\n",
    "                sig = \".\"\n",
    "\n",
    "            print(\"Significance: \", sig)\n",
    "\n",
    "            #d = effsize.cohen_d(R.FloatVector(dfs['human']['{}Sim{}'.format(pair_type, similarity_type)]),\n",
    "            #R.FloatVector(dfs['ai']['{}Sim{}'.format(pair_type, similarity_type)]))\n",
    "\n",
    "            #print(d)\n",
    "            print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############## t-test between 'HumanAISim' vs 'AIHumanSim' ##############\n",
    "\n",
    "lsm_df = pd.DataFrame({'AvgSimSpacy': pd.to_numeric(lsm.data['AvgSimSpacy']),\n",
    "                   'HumanAISimSpacy': pd.to_numeric(lsm.data['HumanAISimSpacy']),\n",
    "                   'AIHumanSimSpacy': pd.to_numeric(lsm.data['AIHumanSimSpacy']),\n",
    "\n",
    "                   'AvgSimValence': pd.to_numeric(lsm.data['AvgSimValence']),\n",
    "                   'HumanAISimValence': pd.to_numeric(lsm.data['HumanAISimValence']),\n",
    "                   'AIHumanSimValence': pd.to_numeric(lsm.data['AIHumanSimValence']),\n",
    "\n",
    "                   'AvgSimTurnLength': pd.to_numeric(lsm.data['AvgSimTurnLength']),\n",
    "                   'HumanAISimTurnLength': pd.to_numeric(lsm.data['HumanAISimTurnLength']),\n",
    "                   'AIHumanSimTurnLength': pd.to_numeric(lsm.data['AIHumanSimTurnLength']),\n",
    "\n",
    "                   'willing_friend': lsm.data['willing_friend'],\n",
    "                   'willing_romantic': lsm.data['willing_romantic'],\n",
    "                   'wtp': lsm.data['wtp'], 'agent_cond': lsm.data['agent_cond'], 'belief': lsm.data['belief']})\n",
    "\n",
    "df_human = lsm_df[lsm_df['agent_cond'] == 'human'] # Human Condition\n",
    "df_ai = lsm_df[lsm_df['agent_cond'] == 'ai'] # AI Condition\n",
    "\n",
    "dfs = {'human': df_human, 'ai': df_ai, 'all': lsm_df}\n",
    "for condition in ['human', 'ai', 'all']:\n",
    "    for dv_type in ['Spacy', 'TurnLength', 'Valence']:\n",
    "        print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-* {} -- {} *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\".format(dv_type, condition))\n",
    "        vt = stats.var_test(R.FloatVector(dfs[condition]['HumanAISim{}'.format(dv_type)]),\n",
    "        R.FloatVector(dfs[condition]['AIHumanSim{}'.format(dv_type)]))\n",
    "\n",
    "\n",
    "        tt = stats.t_test(R.FloatVector(dfs[condition]['HumanAISim{}'.format(dv_type)]),\n",
    "        R.FloatVector(dfs[condition]['AIHumanSim{}'.format(dv_type)]),\n",
    "            **{'var.equal': float(vt.rx('p.value')[0][0])>0.05, 'paired': False})\n",
    "\n",
    "        print(tt)\n",
    "\n",
    "        sig = \"None\"\n",
    "        if tt.rx('p.value')[0][0] < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif tt.rx('p.value')[0][0] < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif tt.rx('p.value')[0][0] < 0.05:\n",
    "            sig = \"*\"\n",
    "        elif tt.rx('p.value')[0][0] < 0.1:\n",
    "            sig = \".\"\n",
    "\n",
    "        print(\"Significance: \", sig)\n",
    "\n",
    "        d = effsize.cohen_d(R.FloatVector(dfs[condition]['HumanAISim{}'.format(dv_type)]),\n",
    "        R.FloatVector(dfs[condition]['AIHumanSim{}'.format(dv_type)]))\n",
    "        print(d)\n",
    "\n",
    "        print(\"Spacy Human-AI M = \", dfs[condition]['{}Spacy'.format(\"HumanAISim\")].mean())\n",
    "        print(\"Spacy Human-AI SD = \", dfs[condition]['{}Spacy'.format(\"HumanAISim\")].std())\n",
    "\n",
    "        print(\"Spacy AI-Human M = \", dfs[condition]['{}Spacy'.format(\"AIHumanSim\")].mean())\n",
    "        print(\"Spacy AI-Human SD = \", dfs[condition]['{}Spacy'.format(\"AIHumanSim\")].std())\n",
    "\n",
    "        print(\"Turn Length Human-AI M = \", dfs[condition]['{}TurnLength'.format(\"HumanAISim\")].mean())\n",
    "        print(\"Turn Length Human-AI SD = \", dfs[condition]['{}TurnLength'.format(\"HumanAISim\")].std())\n",
    "\n",
    "        print(\"Turn Length AI-Human M = \", dfs[condition]['{}TurnLength'.format(\"AIHumanSim\")].mean())\n",
    "        print(\"Turn Length AI-Human SD = \", dfs[condition]['{}TurnLength'.format(\"AIHumanSim\")].std())\n",
    "\n",
    "        print(\"Sentiment Human-AI M = \", dfs[condition]['{}Valence'.format(\"HumanAISim\")].mean())\n",
    "        print(\"Sentiment Human-AI SD = \", dfs[condition]['{}Valence'.format(\"HumanAISim\")].std())\n",
    "\n",
    "        print(\"Sentiment AI-Human M = \", dfs[condition]['{}Valence'.format(\"AIHumanSim\")].mean())\n",
    "        print(\"Sentiment AI-Human SD = \", dfs[condition]['{}Valence'.format(\"AIHumanSim\")].std())\n",
    "\n",
    "        print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ba5ede13f0387f51f50b72b517e398c81deb77e749ad9440ae5ea50ce01832d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
